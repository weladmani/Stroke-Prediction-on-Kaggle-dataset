{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMe3gNX1BWtzlikQyvAJtv5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weladmani/Stroke-Prediction-on-Kaggle-dataset/blob/main/Maman22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Libraries for Basic Data Processing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#Libraries for File I/O Operations\n",
        "import os\n",
        "\n",
        "import requests\n",
        "import itertools\n",
        "\n",
        "#Libraries for Exception Handling\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#Libraries for data pre-processing, visualization and analysis\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.figure_factory as ff\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import matplotlib\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import plot_tree\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.over_sampling import SMOTENC\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "Ytu9-Bu0gVJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Read data and get its structure, features and description\n",
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"fedesoriano/stroke-prediction-dataset\")\n",
        "data = pd.read_csv(os.path.join(path, 'healthcare-dataset-stroke-data.csv'))\n",
        "\n",
        "print(f\"\\nGlimpse Of The Dataset :\")\n",
        "data.head(10)"
      ],
      "metadata": {
        "id": "za6jomWDga3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === STEP 1: Load & Clean Data ===\n",
        "data = data[data['gender'] != 'Other']\n",
        "data.drop(columns=['id'], inplace=True)\n",
        "data['bmi'].fillna(data['bmi'].median(), inplace=True)\n",
        "\n",
        "# === STEP 2: Bin Continuous Variables ===\n",
        "\n",
        "# Age groups\n",
        "data['age_group'] = pd.cut(\n",
        "    data['age'],\n",
        "    bins=[0, 25, 50, 75, 120],\n",
        "    labels=['0-25', '26-50', '51-75', '75+']\n",
        ")\n",
        "\n",
        "# BMI groups\n",
        "data['bmi_group'] = pd.cut(\n",
        "    data['bmi'],\n",
        "    bins=[0, 18.5, 25, 30, 100],\n",
        "    labels=['Underweight', 'Healthy', 'Overweight', 'Obese']\n",
        ")\n",
        "\n",
        "# Glucose level groups\n",
        "data['glucose_group'] = pd.cut(\n",
        "    data['avg_glucose_level'],\n",
        "    bins=[0, 140, 200, data['avg_glucose_level'].max()],\n",
        "    labels=['Normal', 'Prediabetes', 'Diabetes']\n",
        ")\n",
        "\n",
        "# === STEP 3: Drop Original Continuous Columns ===\n",
        "data.drop(columns=['age', 'bmi', 'avg_glucose_level'], inplace=True)"
      ],
      "metadata": {
        "id": "8JkByuP7-G35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === STEP 4: Convert Binary Columns to Boolean ===\n",
        "data['hypertension'] = data['hypertension'].astype(bool)\n",
        "data['heart_disease'] = data['heart_disease'].astype(bool)\n",
        "data['ever_married'] = data['ever_married'].map({'Yes': True, 'No': False})\n",
        "data['stroke'] = data['stroke'].astype(bool)"
      ],
      "metadata": {
        "id": "ek7gIf8tD_D7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === STEP 5: One-Hot Encode Categorical Features ===\n",
        "categorical_cols = ['gender', 'work_type', 'Residence_type', 'smoking_status', 'age_group', 'bmi_group', 'glucose_group']\n",
        "data_encoded = pd.get_dummies(data, columns=categorical_cols)"
      ],
      "metadata": {
        "id": "ektY7FsVEAXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === STEP 6: Prepare for SMOTENC ===\n",
        "X = data_encoded.drop('stroke', axis=1)\n",
        "y = data_encoded['stroke']\n",
        "\n",
        "# Identify which columns are categorical (one-hot encoded start index)\n",
        "categorical_features = [i for i, col in enumerate(X.columns) if any(prefix in col for prefix in [\n",
        "    'gender_', 'work_type_', 'Residence_type_', 'smoking_status_', 'age_group_', 'bmi_group_', 'glucose_group_'\n",
        "])]\n",
        "\n",
        "smote = SMOTENC(categorical_features=categorical_features, random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "# Combine into balanced DataFrame\n",
        "data_balanced = pd.DataFrame(X_resampled, columns=X.columns)\n",
        "data_balanced['stroke'] = y_resampled.astype(bool)"
      ],
      "metadata": {
        "id": "9JiWrZS2EAoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === STEP 7: Convert All Columns to Boolean for Apriori ===\n",
        "data_apriori = data_balanced.astype(bool)\n",
        "\n",
        "# === STEP 8: Run Apriori Algorithm ===\n",
        "freq_items = apriori(data_apriori, min_support=0.4, use_colnames=True)\n",
        "rules = association_rules(freq_items, metric=\"confidence\", min_threshold=0.6)\n",
        "\n",
        "# === STEP 9: Filter or Sort Rules ===\n",
        "rules_filteredall = rules.sort_values(by='confidence', ascending=False)\n",
        "rules_filtered = rules[(rules['lift'] > 1.1)].sort_values(by='confidence', ascending=False)"
      ],
      "metadata": {
        "id": "bohgLG20ECEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === STEP 10: Display All Rules ===\n",
        "print(\"All Association Rules (lift > 1.1):\\n\")\n",
        "for _, row in rules_filteredall.iterrows():\n",
        "    antecedents = ', '.join([str(i) for i in row['antecedents']])\n",
        "    consequents = ', '.join([str(i) for i in row['consequents']])\n",
        "    print(f\"If {{{antecedents}}} ‚Üí {{{consequents}}} | \"\n",
        "          f\"support: {row['support']:.2f}, \"\n",
        "          f\"confidence: {row['confidence']:.2f}, \"\n",
        "          f\"lift: {row['lift']:.2f}\")"
      ],
      "metadata": {
        "id": "wtEvLSWgFVp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === STEP 11: Display Top Rules ===\n",
        "print(\"Top Association Rules (lift > 1.1):\\n\")\n",
        "for _, row in rules_filtered.head(10).iterrows():\n",
        "    antecedents = ', '.join([str(i) for i in row['antecedents']])\n",
        "    consequents = ', '.join([str(i) for i in row['consequents']])\n",
        "    print(f\"If {{{antecedents}}} ‚Üí {{{consequents}}} | \"\n",
        "          f\"support: {row['support']:.2f}, \"\n",
        "          f\"confidence: {row['confidence']:.2f}, \"\n",
        "          f\"lift: {row['lift']:.2f}\")"
      ],
      "metadata": {
        "id": "y4kApp90EIbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# results for question 1\n",
        "\n",
        "The results show clear statistical associations between variables such as age, marital status, and stroke. However, it is important to note that these relationships are likely influenced by a third variable ‚Äî in this case, age. Older individuals are more likely both to be married and to have experienced a stroke."
      ],
      "metadata": {
        "id": "Pxx5bKB-FsZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2 - Clustering Analysis\n",
        "\n",
        "A. To evaluate the quality of the clustering I performed on the Stroke Prediction Dataset, I will use two measures: Silhouette Score and Normalized Mutual Information (NMI).\n",
        "\n",
        "1. **Silhouette Score**: The Silhouette Score is an internal evaluation metric, meaning it only looks at the structure of the clusters, without using the actual labels.\n",
        "It measures how similar each data point is to its own cluster compared to other clusters.\n",
        "The score ranges from -1 to 1, where values closer to 1 indicate well-defined and separated clusters.\n",
        "I used this measure to check whether the clustering was meaningful based on the features in the dataset.\n",
        "\n",
        "2. **Normalized Mutual Information**: NMI is an external metric that compares the cluster labels to the actual labels in the dataset (in this case, whether or not the person had a stroke).\n",
        "It measures how much information is shared between the clusters and the real labels.\n",
        "The score also ranges from 0 to 1, where 1 means a perfect match between the clusters and the actual stroke labels.\n",
        "This helps us understand if the clusters we found are related to the stroke outcome in a meaningful way."
      ],
      "metadata": {
        "id": "-RPrXu0VHHz4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "B. **Clustering Method: K-Means**:\n",
        "K-Means is a popular unsupervised machine learning algorithm that groups the data into K clusters based on similarity.\n",
        "It works by:\n",
        "1. Selecting K cluster centers (called centroids).\n",
        "2. Assigning each data point to the closest centroid.\n",
        "3. Recalculating the centroids based on the assigned points.\n",
        "4. Repeating the process until the assignments no longer change significantly.\n",
        "\n",
        "I chose K-means becuase it is easy to understand, quick to implement, works well with large datasets and is compatibile with our dataset since most features are numeric (after encoding)"
      ],
      "metadata": {
        "id": "HOkThbG5927B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_2 = pd.read_csv(os.path.join(path, 'healthcare-dataset-stroke-data.csv'))\n",
        "\n",
        "# Drop Unnecessary Columns\n",
        "data_2.drop(['id'], axis=1, inplace=True)\n",
        "# Handle Missing Values\n",
        "data_2['bmi'].fillna(data_2['bmi'].median(), inplace=True)\n",
        "# Encode Categorical Variables\n",
        "data_encoded = pd.get_dummies(data_2, drop_first=True)\n",
        "\n",
        "#Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(data_encoded)\n"
      ],
      "metadata": {
        "id": "biiwPGJ7_RuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apply K-Means Clustering\n",
        "I will use the Elbow method to Find the Optimal K"
      ],
      "metadata": {
        "id": "2EKV9umi_pWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "inertia = []\n",
        "K_range = range(1, 11)\n",
        "\n",
        "for k in K_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(data_scaled)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "plt.plot(K_range, inertia, marker='o')\n",
        "plt.xlabel('Number of Clusters (K)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method For Optimal K')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_h2YARpsApul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimal K would be K=3"
      ],
      "metadata": {
        "id": "5dSFKh4cBnzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "clusters = kmeans.fit_predict(data_scaled)\n",
        "\n",
        "# Add the clusters back to the DataFrame:\n",
        "data_2['Cluster'] = clusters\n"
      ],
      "metadata": {
        "id": "dqpP1wBz_oVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Evaluate Clustering Quality\n",
        "\n",
        "sil_score = silhouette_score(data_scaled, clusters)\n",
        "print(f\"Silhouette Score: {sil_score:.2f}\")"
      ],
      "metadata": {
        "id": "UEoO7EUQ_97o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import normalized_mutual_info_score\n",
        "\n",
        "# Normalized Mutual Information\n",
        "\n",
        "nmi_score = normalized_mutual_info_score(data_2['stroke'], data_2['Cluster'])\n",
        "print(f\"NMI Score: {nmi_score:.2f}\")"
      ],
      "metadata": {
        "id": "uxXdTJ3eAKvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reduce data to 2 principal components\n",
        "pca = PCA(n_components=2)\n",
        "pca_components = pca.fit_transform(data_scaled)\n",
        "\n",
        "# Plot clusters in 2D\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(pca_components[:, 0], pca_components[:, 1], c=data_2['Cluster'], cmap='viridis', alpha=0.6)\n",
        "plt.title('K-Means Clusters (K=3) Visualized with PCA')\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-5TbQikcAvr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Average values per cluster\n",
        "summary = data_2.groupby('Cluster').mean(numeric_only=True).round(2)\n",
        "\n",
        "# Stroke proportion per cluster\n",
        "stroke_distribution = data_2.groupby('Cluster')['stroke'].value_counts(normalize=True).unstack().fillna(0)\n",
        "stroke_distribution.columns = ['No Stroke', 'Stroke']\n",
        "\n",
        "\n",
        "print(\"Feature Averages by Cluster:\\n\", summary)\n",
        "print(\"\\nStroke Proportion per Cluster:\\n\", stroke_distribution)"
      ],
      "metadata": {
        "id": "C30yj3I8CeoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final Assumptions**\n",
        "\n",
        "After applying K-Means clustering with K=3, we notice three distinct groups:\n",
        "\n",
        "Cluster 1: Young individuals (avg. age 7), with no hypertension or heart disease, low glucose and BMI. Very low stroke risk (0.3%). This is a low-risk group.\n",
        "\n",
        "Clusters 0 & 2: Middle-aged individuals (avg. age 47-49), with moderate levels of hypertension, heart disease, and higher BMI and glucose. Both show similar stroke rates (~ 5-6%), and represent the moderate- to high-risk groups."
      ],
      "metadata": {
        "id": "F4ceZE3IC2QA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Artificial Neural Network\n",
        "A.\n",
        "Since this is a binary classification problem (stroke = 1 or 0) I will define a feedforward ANN with:\n",
        "\n",
        "\n",
        "    Layer                  | Neurons | Activation | Purpose\n",
        "    -----------------------|---------|------------|----------------\n",
        "    Input + Hidden layer 1 | 32      | ReLU       | Detects patterns in features like age, BMI, etc.\n",
        "    Hidden Layer 2         | 16      | ReLU       | Learns deeper/more abstract\n",
        "    interactions.\n",
        "    Hidden Layer 3         | 8       | ReLU       | Learns deeper/more abstract\n",
        "    interactions.\n",
        "    Output Layer\t       | 1       | Sigmoid    | Outputs probability of stroke (0-1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   All layers are fully connected - each neuron in one layer connects to every neuron in the next.\n",
        "*   The number of input neurons = number of features after preprocessing\n",
        "\n",
        "**Data Flow**\n",
        "\n",
        "1.   Input: A row from the dataset is passed to the first layer.\n",
        "2.   Each neuron computes:\n",
        "      z=‚àë(w\n",
        "i\n",
        "‚Äã\n",
        " ‚ãÖx\n",
        "i\n",
        "‚Äã\n",
        " )+b\n",
        "then applies the activation function to get the output.\n",
        "3. The result flows to the next layer and the process repeats.\n",
        "4. Output neuron returns a probability between 0 and 1.\n",
        "\n",
        "**Activation Functions**\n",
        "\n",
        "* ReLU (Rectified Linear Unit): f(x) = max(0,x)\n",
        "Helps prevent vanishing gradients and speeds up training.\n",
        "* Sigmoid: f(x) = 1 / (1 + e^-x)\n",
        "Used at the output layer to produce a value betweeb 0 and 1 -> perfect for binary classification."
      ],
      "metadata": {
        "id": "IT_zDThgEIAH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "B.\n",
        "* Loss Function: Loss = -[y * log(y^) + (1 - y) * log(1 - y^)]\n",
        "\n",
        "* Batch Size: 32, the number of training samples  processed before updating the model's weights.\n",
        "\n",
        "* Learning Rate: will use Adam optimizer, which automaticallty adjusts the learning arete for each parameter.\n",
        "\n",
        "The optimization process uses binary cross-entropy as the loss function to measure the prediction error for stroke vs. non-stroke.\n",
        "The model is trained using the Adam optimizer, which adaptively adjusts the learning rate during training.\n",
        "I use a batch size of 32, meaning the model updates weights after processing every 32 examples.\n",
        "The default learning rate for Adam is 0.001, which provides a good balance between speed and convergence stability.\n",
        "\n"
      ],
      "metadata": {
        "id": "wOvStj-KMRiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Progress bar callback using tqdm\n",
        "class TQDMProgressBar(Callback):\n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.epochs = self.params['epochs']\n",
        "        self.tqdm = tqdm(total=self.epochs, desc=\"Training Progress\")\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.tqdm.update(1)\n",
        "    def on_train_end(self, logs=None):\n",
        "        self.tqdm.close()\n",
        "\n",
        "# Load and preprocess dataset\n",
        "data = pd.read_csv(os.path.join(path, 'healthcare-dataset-stroke-data.csv'))\n",
        "data.drop(['id'], axis=1, inplace=True)\n",
        "data['bmi'].fillna(data['bmi'].median(), inplace=True)\n",
        "data_encoded = pd.get_dummies(data, drop_first=True)\n",
        "\n",
        "X = data_encoded.drop('stroke', axis=1)\n",
        "y = data_encoded['stroke']\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply SMOTE\n",
        "sm = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = sm.fit_resample(X_scaled, y)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define ANN model\n",
        "model = Sequential([\n",
        "    Dense(32, input_dim=X_train.shape[1], activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(8, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=48,\n",
        "    batch_size=32,\n",
        "    verbose=0,\n",
        "    callbacks=[TQDMProgressBar()]\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "y_pred_prob = model.predict(X_test).flatten()\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "print(\"‚úÖ Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"‚úÖ Recall (stroke cases):\", recall_score(y_test, y_pred))\n",
        "print(\"\\nüßæ Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"üìâ Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Plot Loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Binary Cross-Entropy Loss')\n",
        "plt.title('Training and Test Loss Over Epochs (With SMOTE)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6siwYSn1PwB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the Loss is decreasing with every Epoch."
      ],
      "metadata": {
        "id": "93VL_z7kdMZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get indices of false positives: predicted 1, actual 0\n",
        "# Reset index of y_test to align with X_test_unscaled\n",
        "false_positive_idx = (y_pred == 1) & (y_test.reset_index(drop=True) == 0)\n",
        "\n",
        "# Recover original feature names from the encoded data before scaling\n",
        "feature_names = X.columns\n",
        "\n",
        "# Inverse transform to get original (unscaled) feature values\n",
        "# Fit the scaler on the scaled training data used for the model\n",
        "scaler.fit(X_scaled)\n",
        "X_test_unscaled = pd.DataFrame(scaler.inverse_transform(X_test), columns=feature_names)\n",
        "\n",
        "# Get corresponding false positive rows\n",
        "false_positives = X_test_unscaled[false_positive_idx].copy()\n",
        "false_positives['Predicted_Prob'] = y_pred_prob[false_positive_idx]\n",
        "\n",
        "# Print all false positives\n",
        "print(f\"\\nüîé Total False Positives: {false_positives.shape[0]}\\n\")\n",
        "from IPython.display import display\n",
        "display(false_positives.head(10))\n",
        "\n",
        "# Get indices of false negatives: predicted 0, actual 1\n",
        "false_negative_idx = (y_pred == 0) & (y_test.reset_index(drop=True) == 1)\n",
        "\n",
        "# Inverse transform to get original (unscaled) feature values\n",
        "false_negatives = X_test_unscaled[false_negative_idx].copy()\n",
        "false_negatives['Predicted_Prob'] = y_pred_prob[false_negative_idx]\n",
        "\n",
        "# Print all false negatives\n",
        "print(f\"\\n‚ö†Ô∏è Total False Negatives: {false_negatives.shape[0]}\\n\")\n",
        "display(false_negatives.head(10))"
      ],
      "metadata": {
        "id": "OeitEqAbRXC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**analyzing and assumptions**\n",
        "\n",
        "C. The final ANN model achieved 87% accuracy with an excellent 97% recall on stroke cases. This suggests the model is highly effective at identifying patients at risk, with a low false negative rate. While false positives are present (~20%), this is often acceptable in medical screening where early detection is prioritized. The model was trained on SMOTE-balanced data, and both training and test loss show stable convergence without overfitting. Future improvements can aim to refine precision while preserving this strong sensitivity."
      ],
      "metadata": {
        "id": "Uzs1BOoEc01G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Summary ‚Äì Stroke Prediction Project\n",
        "\n",
        "This project combined classification models, association rule mining, clustering, and neural networks to explore and predict stroke risk in a real-world medical dataset.\n",
        "\n",
        "**Phase 1: Supervised Learning**\n",
        "\n",
        "\n",
        "*   Random Forest and Logistic Regression were applied initially.\n",
        "*   Logistic Regression was selected as the best model due to its strong recall and balanced performance, making it suitable for stroke detection where missing a case is critical.\n",
        "\n",
        "**Phase 2: Association Rule Mining**\n",
        "Key patterns revealed:\n",
        "\n",
        "\n",
        "*   Stroke ‚Üí Ever Married (Confidence: 97%, Lift: 1.20)\n",
        "*   Age 51‚Äì75 ‚Üí Ever Married (Confidence: 96%)\n",
        "*   Ever Married ‚Üí Stroke (Confidence: 60%, Lift: 1.20)\n",
        "\n",
        "These rules suggest that age is a hidden confounder, influencing both marital status and stroke likelihood.\n",
        "\n",
        "**Phase 3: Clustering (K-Means, K=3)**\n",
        "Identified three distinct groups:\n",
        "\n",
        "\n",
        "*   Low-risk: Very young, no health risk factors.\n",
        "*   Moderate- to high-risk: Middle-aged, higher BMI, glucose, and comorbidities.\n",
        "\n",
        "Reinforced age, BMI, and hypertension as key stroke predictors.\n",
        "\n",
        "\n",
        "**Phase 4: ANN + SMOTE**\n",
        "Final results:\n",
        "* Accuracy: 87%\n",
        "* Recall (stroke cases): 98%\n",
        "* Precision: 80%\n",
        "* Only 19 false negatives ‚Üí strong detection power\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "P0L2z7xhdl0s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Conclusion\n",
        "\n",
        "Based on all the results, the ANN with SMOTE is the strongest model tested so far, especially for high recall, making it the most suitable for stroke risk detection."
      ],
      "metadata": {
        "id": "JtU9hDprjhgE"
      }
    }
  ]
}
